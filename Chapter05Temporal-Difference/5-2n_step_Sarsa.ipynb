{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# n-step-Sarsa 对应《强化学习的数学原理》第7章",
   "id": "27258727a4090e2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# n-step-Sarsa更新公式的推导\n",
    "具体数学推导请看《强化学习的数学原理》第7章\n",
    "<br>代码对应《动手学强化学习》第5章 [请点击这里](https://hrl.boyuai.com/chapter/1/%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95)"
   ],
   "id": "4fae1e2b8fbe9d36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"./picture5_2.png\" alt=\"插入图片哈哈\" width=\"50%\">",
   "id": "9b78a98be8e67a0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 环境\n",
    "还是悬崖漫步环境"
   ],
   "id": "faf4768c092f1a63"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-28T12:47:13.094620Z",
     "start_time": "2025-12-28T12:47:13.087541Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "class CliffWalkingEnv:\n",
    "    def __init__(self, ncol, nrow):\n",
    "        self.nrow = nrow\n",
    "        self.ncol = ncol\n",
    "        self.x = 0              # 记录当前智能体位置的横坐标  。初始位置在左下角\n",
    "        self.y = self.nrow - 1  # 记录当前智能体位置的纵坐标\n",
    "\n",
    "    def step(self, action):  # 外部调用这个函数来改变当前位置\n",
    "        # 4种动作, change[0]:上, change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)\n",
    "        # 定义在左上角\n",
    "        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        self.x = min(self.ncol - 1, max(0, self.x + change[action][0]))\n",
    "        self.y = min(self.nrow - 1, max(0, self.y + change[action][1]))\n",
    "        next_state = self.y * self.ncol + self.x\n",
    "        reward = -1\n",
    "        done = False\n",
    "        if self.y == self.nrow - 1 and self.x > 0:  # 下一个位置在悬崖或者目标\n",
    "            done = True\n",
    "            if self.x != self.ncol - 1:\n",
    "                reward = -100\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def reset(self):  # 回归初始状态,坐标轴原点在左上角\n",
    "        self.x = 0\n",
    "        self.y = self.nrow - 1\n",
    "        return self.y * self.ncol + self.x"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## n-step-Sarsa",
   "id": "c4923faa8bd33bf7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T12:47:13.138400Z",
     "start_time": "2025-12-28T12:47:13.125019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class nstep_Sarsa:\n",
    "    \"\"\" n步Sarsa算法 \"\"\"\n",
    "    def __init__(self, n, ncol, nrow, epsilon, alpha, gamma, n_action=4):\n",
    "        self.Q_table = np.zeros([nrow * ncol, n_action])\n",
    "        self.n_action = n_action\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.n = n             # 采用n步Sarsa算法\n",
    "        self.state_list = []   # n-step-Sarsa,用来保存这n步的状态\n",
    "        self.action_list = []  # 保存动作\n",
    "        self.reward_list = []  # 保存奖励\n",
    "\n",
    "    def take_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.n_action)\n",
    "        else:\n",
    "            action = np.argmax(self.Q_table[state])\n",
    "        return action\n",
    "\n",
    "    def best_action(self, state):  # 用于打印策略，训练的过程中不用\n",
    "        Q_max = np.max(self.Q_table[state])\n",
    "        a = [0 for _ in range(self.n_action)]\n",
    "        for i in range(self.n_action):\n",
    "            if self.Q_table[state, i] == Q_max:\n",
    "                a[i] = 1\n",
    "        return a\n",
    "\n",
    "    def update(self, s0, a0, r, s1, a1, done):\n",
    "        self.state_list.append(s0)\n",
    "        self.action_list.append(a0)\n",
    "        self.reward_list.append(r)\n",
    "        if len(self.state_list) == self.n:  # 若保存的数据可以进行n步更新，state_list存够n个了才进行\n",
    "            G = self.Q_table[s1, a1]        # 得到Q(s_{t+n}, a_{t+n})\n",
    "            for i in reversed(range(self.n)):\n",
    "                G = self.gamma * G + self.reward_list[i]  # 不断向前计算每一步的回报\n",
    "\n",
    "                if done == True and i > 0:  # 如果这里episode已经结束了，但是i还没有到0，说明什么，后面不够n步但是episode已经结束了\n",
    "                    s = self.state_list[i]  # 那么就用剩下的那几步来更新\n",
    "                    a = self.action_list[i]\n",
    "                    self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])\n",
    "\n",
    "            # 更新q\n",
    "            s = self.state_list[0]\n",
    "            a = self.action_list[0]\n",
    "            self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])\n",
    "\n",
    "            # 删除已经更新的，即多步中的第一个\n",
    "            self.state_list.pop(0)\n",
    "            self.action_list.pop(0)\n",
    "            self.reward_list.pop(0)\n",
    "\n",
    "        if done == True:  # 如果到达终止状态,即将开始下一条序列,则将列表全清空\n",
    "            self.state_list = []\n",
    "            self.action_list = []\n",
    "            self.reward_list = []"
   ],
   "id": "f031c774691a6e77",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 训练",
   "id": "f64f240216ad5f0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 实例化",
   "id": "d1fddc733c654f41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T12:47:13.170860Z",
     "start_time": "2025-12-28T12:47:13.141394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ncol = 12\n",
    "nrow = 4\n",
    "env = CliffWalkingEnv(ncol, nrow)\n",
    "np.random.seed(0)\n",
    "n_step = 5    # 5步Sarsa算法\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "agent = nstep_Sarsa(n_step, ncol, nrow, epsilon, alpha, gamma)"
   ],
   "id": "d656fd6b0af75739",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 开始训练",
   "id": "dc8925ea5a508917"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T12:47:13.896978Z",
     "start_time": "2025-12-28T12:47:13.184096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_episodes = 2000\n",
    "return_list = []\n",
    "for i in range(num_episodes):\n",
    "    episode_return = 0\n",
    "    state = env.reset()\n",
    "    action = agent.take_action(state)\n",
    "    done = False\n",
    "    while done == False:\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_action = agent.take_action(next_state)\n",
    "        episode_return += reward\n",
    "        agent.update(state, action, reward, next_state, next_action,done)\n",
    "        #调用update,就开始把s,a,r传进列表，所以这个对应的是新的，也就是后面的\n",
    "        #一开始不够n个时，就不计算不更新，直到传够5个了再开始算，所以当不够n个时，update就只传\n",
    "        #此时还在while循环里，直到传够了，开始算了才会出现done=Ture\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "    return_list.append(episode_return)"
   ],
   "id": "f6961ba92d9e2d95",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 训练结束",
   "id": "d9ba50c55c643872"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 打印q表",
   "id": "1a701214f76da493"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T12:47:13.926935Z",
     "start_time": "2025-12-28T12:47:13.911404Z"
    }
   },
   "cell_type": "code",
   "source": "agent.Q_table",
   "id": "4ac8cb6af8e23562",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -8.67124173,  -8.94154735,  -8.83455162,  -8.00642999],\n",
       "       [ -8.55453599,  -8.51395348,  -8.5663023 ,  -7.83402992],\n",
       "       [ -8.32748643,  -8.89485253,  -8.45162647,  -7.57242019],\n",
       "       [ -7.75715682,  -8.11546513,  -8.00649119,  -7.10185879],\n",
       "       [ -8.531435  ,  -8.26629271,  -7.91088849,  -6.78081316],\n",
       "       [ -7.22722434,  -8.32551707,  -7.93510965,  -6.30315173],\n",
       "       [ -7.12117162,  -7.24727071,  -7.23725892,  -5.86014523],\n",
       "       [ -6.69694967,  -7.71968737,  -6.9547111 ,  -5.36068139],\n",
       "       [ -6.45075368,  -6.28342993,  -6.43214199,  -4.77878561],\n",
       "       [ -5.01249857,  -6.3181405 ,  -6.31587254,  -4.18353934],\n",
       "       [ -4.96363731,  -3.97572523,  -5.355453  ,  -3.54703933],\n",
       "       [ -4.270789  ,  -2.82146915,  -4.63821007,  -4.20049381],\n",
       "       [ -8.26545803,  -9.75824778,  -8.94944896,  -9.48025437],\n",
       "       [ -8.2327698 , -10.37261831, -15.06644406, -15.18647659],\n",
       "       [ -7.99701568, -22.25696147, -11.2487198 , -10.73462018],\n",
       "       [ -7.76388045, -11.73390091, -10.81697039, -11.37521976],\n",
       "       [-15.72146871, -16.82663555, -10.20633542,  -7.70817365],\n",
       "       [ -9.88371498, -10.02683652,  -9.89023086,  -7.16746668],\n",
       "       [ -6.5802476 , -13.650459  , -10.18756518, -12.06727817],\n",
       "       [-10.03367279, -22.74107151, -15.52494259,  -6.40834063],\n",
       "       [ -7.98840056,  -8.60821696,  -8.47642192,  -5.82173853],\n",
       "       [ -5.19499371,  -8.28709785,  -7.00178079,  -5.32739563],\n",
       "       [ -5.36650037, -11.02535874,  -6.59298196,  -2.84305202],\n",
       "       [ -4.09584517,  -1.90975735,  -3.90105418,  -2.86049149],\n",
       "       [ -8.45205635, -12.4116845 , -10.53949441, -11.07707388],\n",
       "       [-18.03113147, -52.17031   ,  -9.50365253, -16.16994005],\n",
       "       [ -8.82198176, -34.39      , -14.09632739, -24.19399491],\n",
       "       [-13.89652687, -46.8559    ,  -9.85447406,  -9.8230298 ],\n",
       "       [ -9.4680046 , -27.1       , -17.47835466, -14.17470658],\n",
       "       [ -9.37703658, -10.        , -17.70374169, -13.94083853],\n",
       "       [ -9.78754282, -27.1       ,  -9.23079688, -15.87234566],\n",
       "       [-16.95634906, -34.39      , -14.19357939, -10.06563954],\n",
       "       [ -6.14244997, -19.        ,  -9.95840907, -16.10952249],\n",
       "       [ -4.87752171, -19.        , -10.27619876, -10.43408539],\n",
       "       [ -2.49057231, -40.951     ,  -2.94601054,  -1.98934106],\n",
       "       [ -2.84222016,  -1.        ,  -3.62402223,  -1.96847115],\n",
       "       [ -8.66323537, -18.34501588, -11.54477439, -56.953279  ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 策略可视化",
   "id": "2f1f167fa3efad12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T12:47:13.991634Z",
     "start_time": "2025-12-28T12:47:13.977650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_agent(agent, env, action_meaning, disaster=[], end=[]):\n",
    "    for i in range(env.nrow):\n",
    "        for j in range(env.ncol):\n",
    "            if (i * env.ncol + j) in disaster:\n",
    "                print('****', end=' ')\n",
    "            elif (i * env.ncol + j) in end:\n",
    "                print('EEEE', end=' ')\n",
    "            else:\n",
    "                a = agent.best_action(i * env.ncol + j)   #根据q-table输出策略\n",
    "                pi_str = ''\n",
    "                for k in range(len(action_meaning)):\n",
    "                    pi_str += action_meaning[k] if a[k] > 0 else 'o'\n",
    "                print(pi_str, end=' ')\n",
    "        print()"
   ],
   "id": "3c61549bcd785592",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T12:47:14.053021Z",
     "start_time": "2025-12-28T12:47:14.038021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "action_meaning = ['^', 'v', '<', '>']\n",
    "print('5步Sarsa算法最终收敛得到的策略为：')\n",
    "print_agent(agent, env, action_meaning, list(range(37, 47)), [47])"
   ],
   "id": "9aeb0033807584cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5步Sarsa算法最终收敛得到的策略为：\n",
      "ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ovoo \n",
      "^ooo ^ooo ^ooo ^ooo ooo> ooo> ^ooo ooo> ooo> ^ooo ooo> ovoo \n",
      "^ooo oo<o ^ooo ooo> ^ooo ^ooo oo<o ooo> ^ooo ^ooo ooo> ovoo \n",
      "^ooo **** **** **** **** **** **** **** **** **** **** EEEE \n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T12:47:14.084243Z",
     "start_time": "2025-12-28T12:47:14.075022Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e8c6d81f5f4ab997",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (mydrl01)",
   "language": "python",
   "name": "mydrl01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
