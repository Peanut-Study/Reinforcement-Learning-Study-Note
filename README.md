# Reinforcement-Learning-Study-Note
è¿™æ˜¯æˆ‘çš„å¼ºåŒ–å­¦ä¹ ç¬”è®°
<br>This is my reinforcement learning study note
<br>æœ¬ç¡•éè®¡ç®—æœºç›¸å…³ä¸“ä¸šï¼Œè‡ªå­¦å¼ºåŒ–å­¦ä¹ ï¼Œæ¬¢è¿è¯„è®ºäº¤æµï¼Œæ„Ÿè°¢æ‰¹è¯„æŒ‡æ­£ï¼
## å‚è€ƒèµ„æ–™
| åºå· | èµ„æ–™åç§° | é“¾æ¥ |
|---|---|---|
| 1 | å´æ©è¾¾æœºå™¨å­¦ä¹  | [è§†é¢‘](https://www.bilibili.com/video/BV1owrpYKEtP/?spm_id_from=333.337.search-card.all.click&vd_source=fea2c3c140631e78a73b6d714dcf9f71) |
| 2 | è¥¿æ¹–å¤§å­¦èµµä¸–é’°ã€Šå¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸç†ã€‹ | [è§†é¢‘](https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.337.search-card.all.click&vd_source=fea2c3c140631e78a73b6d714dcf9f71) [èµ„æ–™](https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning) |
| 3 | ä¸Šæµ·äº¤é€šå¤§å­¦å¼ ä¼Ÿæ¥ ã€ŠåŠ¨æ‰‹å­¦å¼ºåŒ–å­¦ä¹ ã€‹ | [èµ„æ–™](https://hrl.boyuai.com/chapter/intro) |
| 4 | è˜‘è‡ä¹¦å¼ºåŒ–å­¦ä¹  | [èµ„æ–™](https://datawhalechina.github.io/easy-rl/#/) |
## ç¬”è®°ç›®å½•
| ç« èŠ‚ | ç®€ä»‹ | çŠ¶æ€ğŸ”´ğŸŸ¡ğŸŸ¢ | é“¾æ¥ |
| ---- | ---- | ---- | ---- |
| ç¬¬ä¸€ç«  | çŠ¶æ€å€¼ | ğŸŸ¢å·²å®Œæˆ | [ç¬”è®°](https://github.com/Peanut-Study/Reinforcement-Learning-Study-Note/tree/main/Chapter01) |
| ç¬¬äºŒç«  | åŠ¨æ€è§„åˆ’ |  |  |
| ç¬¬ä¸‰ç«  | è’™ç‰¹å¡æ´›æ–¹æ³• | ğŸŸ¢å·²å®Œæˆ | [ç¬”è®°](https://github.com/Peanut-Study/Reinforcement-Learning-Study-Note/tree/main/Chapter03Monte-Carlo) |
| ç¬¬å››ç«  | éšæœºè¿‘ä¼¼ |  |  |
| ç¬¬äº”ç«  | æ—¶åºå·®åˆ† | ğŸŸ¢å·²å®Œæˆ | [ç¬”è®°](https://github.com/Peanut-Study/Reinforcement-Learning-Study-Note/tree/main/Chapter05Temporal-Difference) |
| ç¬¬å…­ç«  | Dyna-Q |  |  |
| ç¬¬ä¸ƒç«  | DQN | ğŸŸ¢å·²å®Œæˆ | [ç¬”è®°](https://github.com/Peanut-Study/Reinforcement-Learning-Study-Note/tree/main/Chapter07DQN) |
| ç¬¬å…«ç«  | æ”¹è¿›çš„DQN | ğŸŸ¢è¿›è¡Œä¸­ | [ç¬”è®°](https://github.com/Peanut-Study/Reinforcement-Learning-Study-Note/tree/main/Chapter08Improved-DQN) |
| ç¬¬ä¹ç«  | ç­–ç•¥æ¢¯åº¦ | ğŸŸ¢å·²å®Œæˆ | [ç¬”è®°](https://github.com/Peanut-Study/Reinforcement-Learning-Study-Note/tree/main/Chapter09Policy-Gradient) |
| ç¬¬åç«  | æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³• | ğŸŸ¡è¿›è¡Œä¸­ | [ç¬”è®°](https://github.com/Peanut-Study/Reinforcement-Learning-Study-Note/tree/main/Chapter10Actor-Critic) |
| ç¬¬åä¸€ç«  | ä¿¡ä»»åŒºåŸŸç­–ç•¥ä¼˜åŒ– |  |  |
| ç¬¬åäºŒç«  | è¿‘ç«¯ç­–ç•¥ä¼˜åŒ– | ğŸŸ¢å·²å®Œæˆ | [ç¬”è®°](https://github.com/Peanut-Study/Reinforcement-Learning-Study-Note/tree/main/Chapter12PPO) |
| ç¬¬åä¸‰ç«  | æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ | ğŸŸ¢å·²å®Œæˆ | [ç¬”è®°](https://github.com/Peanut-Study/Reinforcement-Learning-Study-Note/tree/main/Chapter13DDPG) |
| ç¬¬åå››ç«  | Soft Actor-Critic |  |  |
