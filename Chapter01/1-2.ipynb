{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 求每个状态的state value（用解析解求解贝尔曼方程，得到state value）",
   "id": "5460f69c79da31be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 问题描述\n",
    "如图对应《动手学强化学习》第三章Page26,求出某个策略下每个状态s的state value [请点击这里](https://hrl.boyuai.com/chapter/1/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B)"
   ],
   "id": "53dd031383a0df0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"./picture1_2.png\" alt=\"插入图片哈哈\" width=\"50%\">",
   "id": "62b5172fcdffe598"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 根据上图创建环境",
   "id": "41bc9f9a55bf1915"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:49:57.482333Z",
     "start_time": "2025-12-28T11:49:57.464030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "S = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态集合\n",
    "A = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作集合\n",
    "# 状态转移函数 , 这里的前往是确定性的状态转移，概率前往是随机选状态转移，对应赵书Page3\n",
    "P = {\"s1-保持s1-s1\": 1.0,\n",
    "    \"s1-前往s2-s2\": 1.0,\n",
    "    \"s2-前往s1-s1\": 1.0,\n",
    "    \"s2-前往s3-s3\": 1.0,\n",
    "    \"s3-前往s4-s4\": 1.0,\n",
    "    \"s3-前往s5-s5\": 1.0,\n",
    "    \"s4-前往s5-s5\": 1.0,\n",
    "    \"s4-概率前往-s2\": 0.2,\n",
    "    \"s4-概率前往-s3\": 0.4,\n",
    "    \"s4-概率前往-s4\": 0.4}\n",
    "# 奖励函数\n",
    "R = {\"s1-保持s1\": -1,\n",
    "    \"s1-前往s2\": 0,\n",
    "    \"s2-前往s1\": -1,\n",
    "    \"s2-前往s3\": -2,\n",
    "    \"s3-前往s4\": -2,\n",
    "    \"s3-前往s5\": 0,\n",
    "    \"s4-前往s5\": 10,\n",
    "    \"s4-概率前往\": 1}\n",
    "gamma = 0.5  # 折扣因子\n",
    "MDP = (S, A, P, R, gamma)"
   ],
   "id": "a310bc9d5e780857",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Policy1",
   "id": "479ae5c609870a02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:49:57.494457Z",
     "start_time": "2025-12-28T11:49:57.487490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Pi_1 = {\"s1-保持s1\": 0.5,\n",
    "        \"s1-前往s2\": 0.5,\n",
    "        \"s2-前往s1\": 0.5,\n",
    "        \"s2-前往s3\": 0.5,\n",
    "        \"s3-前往s4\": 0.5,\n",
    "        \"s3-前往s5\": 0.5,\n",
    "        \"s4-前往s5\": 0.5,\n",
    "        \"s4-概率前往\": 0.5}"
   ],
   "id": "7de94a24f70ffa78",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 求解Policy1情况下的state value\n",
   "id": "b9ec1aa96b10d6ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:49:57.521098Z",
     "start_time": "2025-12-28T11:49:57.506323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#根据这个策略1，可以写出状态转移矩阵\n",
    "P_from_mdp_to_mrp = [\n",
    "    [0.5, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.5, 0.5],\n",
    "    [0.0, 0.1, 0.2, 0.2, 0.5],    #P(s4->s1)=0,P(s4->s2)=0.5*0.2=0.1,P(s4->s3)=0.5*0.4=0.2,P(s4->s4)=0.5*0.4=0.2,P(s4->s5)=0.5\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "#转成二维矩阵\n",
    "P_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)"
   ],
   "id": "d3177fe5dd56b949",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:49:57.550268Z",
     "start_time": "2025-12-28T11:49:57.535304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#根据策略1，写出奖励\n",
    "#s1的reward，0.5*(-1)+0.5*0=-0.5\n",
    "#s2的reward，0.5*(-1)+0.5*(-2)=-1.5\n",
    "#s3的reward,0.5*(-2)+0.5*0=-1\n",
    "#s4的reward,0.5*1+0.5*10=5.5\n",
    "#s5的reward,1*0=0\n",
    "R_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]"
   ],
   "id": "71f8678496dd5878",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:49:57.579158Z",
     "start_time": "2025-12-28T11:49:57.566162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# state value的解析解\n",
    "def compute(P, rewards, gamma, states_num):\n",
    "    ''' 利用贝尔曼方程的矩阵形式计算解析解,states_num是MRP的状态数 '''\n",
    "    rewards = np.array(rewards).reshape((-1, 1))  #将rewards写成列向量形式\n",
    "    value = np.dot( np.linalg.inv(np.eye(states_num, states_num) - gamma * P) , rewards )\n",
    "    return value"
   ],
   "id": "b5ccd8d8d26a0e42",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:49:57.610872Z",
     "start_time": "2025-12-28T11:49:57.596047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "V_pi1 = compute(P=P_from_mdp_to_mrp, rewards=R_from_mdp_to_mrp, gamma=0.5, states_num=5)\n",
    "print(V_pi1)"
   ],
   "id": "66369e78775369d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.22555411]\n",
      " [-1.67666232]\n",
      " [ 0.51890482]\n",
      " [ 6.0756193 ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Policy2",
   "id": "30af1e59815d66c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:49:57.641863Z",
     "start_time": "2025-12-28T11:49:57.627864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 策略2\n",
    "Pi_2 = {\"s1-保持s1\": 0.6,\n",
    "        \"s1-前往s2\": 0.4,\n",
    "        \"s2-前往s1\": 0.3,\n",
    "        \"s2-前往s3\": 0.7,\n",
    "        \"s3-前往s4\": 0.5,\n",
    "        \"s3-前往s5\": 0.5,\n",
    "        \"s4-前往s5\": 0.1,\n",
    "        \"s4-概率前往\": 0.9}"
   ],
   "id": "1e49cbab42d2cea2",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 求解Policy2情况下的state value",
   "id": "18e9deb728c1b8f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:49:57.673121Z",
     "start_time": "2025-12-28T11:49:57.657972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "P2_from_mdp_to_mrp = [\n",
    "    [0.6,0.4,0,0,0],\n",
    "    [0.3,0,0.7,0,0],\n",
    "    [0,0,0,0.5,0.5],\n",
    "    [0,0.18,0.36,0.36,0.1],\n",
    "    [0,0,0,0,1]\n",
    "]\n",
    "P2_from_mdp_to_mrp = np.array(P2_from_mdp_to_mrp)"
   ],
   "id": "7d78a7dfebc18688",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:49:57.704586Z",
     "start_time": "2025-12-28T11:49:57.690506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#根据策略2，写出奖励\n",
    "#s1的reward，0.6*(-1)+0.4*0=-0.6\n",
    "#s2的reward，0.3*(-1)+0.7*(-2)=-1.7\n",
    "#s3的reward,0.5*(-2)+0.5*0=-1\n",
    "#s4的reward,0.9*1+0.1*10=1.9\n",
    "#s5的reward,0\n",
    "R2_from_mdp_to_mrp = [-0.6,-1.7,-1,1.9,0]"
   ],
   "id": "1227e144097768c",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:49:57.736585Z",
     "start_time": "2025-12-28T11:49:57.722587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "V_pi2 = compute(P=P2_from_mdp_to_mrp, rewards=R2_from_mdp_to_mrp, gamma=0.5, states_num=5)\n",
    "print(V_pi2)"
   ],
   "id": "3a035b00933dfbb5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.45585051]\n",
      " [-2.09547678]\n",
      " [-0.50599771]\n",
      " [ 1.97600915]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:49:57.767585Z",
     "start_time": "2025-12-28T11:49:57.752585Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8cb8671eac6a2e8b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch01)",
   "language": "python",
   "name": "torch01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
