{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 求每个状态的state value(用矩阵运算求解贝尔曼方程，得到state value，解析解)",
   "id": "26c675418cede5fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 问题描述\n",
    "如图对应《强化学习的数学原理》第二章2.5示例Page22,求出某个策略下每个状态s的state value"
   ],
   "id": "57bc3a8e2297aaba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"./picture1_3.png\" alt=\"插入图片哈哈\" width=\"40%\">",
   "id": "9b71c6a2b2cd37c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 根据上图创建环境",
   "id": "34f39ed9075a6d60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:01.673910Z",
     "start_time": "2025-12-28T11:53:01.659799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "S = [\"s1\", \"s2\", \"s3\", \"s4\"]  # 状态集合\n",
    "A = [\"a1\", \"a2\", \"a3\", \"a4\", \"a5\"]  # 动作集合,上，下，左，右，原地\n",
    "# 状态转移函数\n",
    "P = {   \"s1-a1-s1\": 1.0,\n",
    "        \"s1-a2-s2\": 1.0,\n",
    "        \"s1-a3-s3\": 1.0,\n",
    "        \"s1-a4-s1\": 1.0,\n",
    "        \"s1-a5-s1\": 1.0,\n",
    "\n",
    "        \"s2-a1-s2\": 1.0,\n",
    "        \"s2-a2-s2\": 1.0,\n",
    "        \"s2-a3-s4\": 1.0,\n",
    "        \"s2-a4-s1\": 1.0,\n",
    "        \"s2-a5-s2\": 1.0,\n",
    "\n",
    "        \"s3-a1-s1\": 1.0,\n",
    "        \"s3-a2-s4\": 1.0,\n",
    "        \"s3-a3-s3\": 1.0,\n",
    "        \"s3-a4-s3\": 1.0,\n",
    "        \"s3-a5-s3\": 1.0,\n",
    "\n",
    "        \"s4-a1-s2\": 1.0,\n",
    "        \"s4-a2-s4\": 1.0,\n",
    "        \"s4-a3-s4\": 1.0,\n",
    "        \"s4-a4-s3\": 1.0,\n",
    "        \"s4-a5-s4\": 1.0}\n",
    "# 奖励函数\n",
    "R = {\"s1-a1\": 0,\n",
    "    \"s1-a2\": -1,\n",
    "    \"s1-a3\": 0,\n",
    "    \"s1-a4\": 0,\n",
    "    \"s1-a5\": 0,\n",
    "    \"s2-a1\": -1,\n",
    "    \"s2-a2\": -1,\n",
    "    \"s2-a3\": 1,\n",
    "    \"s2-a4\": 0,\n",
    "    \"s2-a5\": -1,\n",
    "    \"s3-a1\": 0,\n",
    "    \"s3-a2\": 1,\n",
    "    \"s3-a3\": 0,\n",
    "    \"s3-a4\": 0,\n",
    "    \"s3-a5\": 0,\n",
    "    \"s4-a1\": -1,\n",
    "    \"s4-a2\": 0,\n",
    "    \"s4-a3\": 0,\n",
    "    \"s4-a4\": 0,\n",
    "    \"s4-a5\": 1}\n",
    "gamma = 0.5  # 折扣因子\n",
    "MDP = (S, A, P, R, gamma)"
   ],
   "id": "ced8628230b64634",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Policy1(对应P22例子1)",
   "id": "b25c2678a87a694b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:01.689548Z",
     "start_time": "2025-12-28T11:53:01.677652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Pi_1 = {\"s1-a3\": 1,\n",
    "        \"s2-a3\": 1,\n",
    "        \"s3-a2\": 1,\n",
    "        \"s4-a5\": 1,}   #这是一个确定性策略 deterministic policy"
   ],
   "id": "735b029cd5bb1d12",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 解析解求解Policy1情况下的state value",
   "id": "1046cf7dbe518a81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:01.721493Z",
     "start_time": "2025-12-28T11:53:01.706480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "P_from_mdp_to_mrp=[\n",
    "    [0,0,1,0],\n",
    "    [0,0,0,1],\n",
    "    [0,0,0,1],\n",
    "    [0,0,0,1]\n",
    "]\n",
    "#转成二维矩阵\n",
    "P_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)"
   ],
   "id": "5cfc97d69e750b12",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:01.753487Z",
     "start_time": "2025-12-28T11:53:01.739480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#根据策略1，写出奖励\n",
    "#s1的reward，0\n",
    "#s2的reward，1\n",
    "#s3的reward,1\n",
    "#s4的reward,1\n",
    "R_from_mdp_to_mrp = [0,1,1,1]"
   ],
   "id": "fb4c511965c7731b",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:01.782541Z",
     "start_time": "2025-12-28T11:53:01.768135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute(P, rewards, gamma, states_num):\n",
    "    ''' 利用贝尔曼方程的矩阵形式计算解析解,states_num是MRP的状态数 '''\n",
    "    rewards = np.array(rewards).reshape((-1, 1))  #将rewards写成列向量形式\n",
    "    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P),\n",
    "                   rewards)\n",
    "    return value"
   ],
   "id": "5a96c1b09d5d28d2",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:01.812738Z",
     "start_time": "2025-12-28T11:53:01.797294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "V_pi1 = compute(P=P_from_mdp_to_mrp, rewards=R_from_mdp_to_mrp, gamma=0.9, states_num=4)\n",
    "print(V_pi1)  #s1-s5每个状态的state value"
   ],
   "id": "9de79ce64c69b0df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.]\n",
      " [10.]\n",
      " [10.]\n",
      " [10.]]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Policy2(对应P24例子2)",
   "id": "7de5a5b1776e1991"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:01.843129Z",
     "start_time": "2025-12-28T11:53:01.829052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Pi_2 = {\"s1-a3\": 0.5,\n",
    "        \"s1-a2\": 0.5,\n",
    "        \"s2-a3\": 1,\n",
    "        \"s3-a2\": 1,\n",
    "        \"s4-a5\": 1}   #这是一个随机选策略 stochastic policy"
   ],
   "id": "e328272bef0e9cae",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 解析解求解Policy2情况下的state value",
   "id": "383a04cdc741f2c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:01.874425Z",
     "start_time": "2025-12-28T11:53:01.859520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "P2_from_mdp_to_mrp=[\n",
    "    [0,0.5,0.5,0],\n",
    "    [0,0,0,1],\n",
    "    [0,0,0,1],\n",
    "    [0,0,0,1]\n",
    "]\n",
    "#转成二维矩阵\n",
    "P2_from_mdp_to_mrp = np.array(P2_from_mdp_to_mrp)"
   ],
   "id": "137367235d33f1c8",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:01.904584Z",
     "start_time": "2025-12-28T11:53:01.889890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#根据策略2，写出奖励\n",
    "#s1的reward，0.5*(-1)+0.5*0=-0.5\n",
    "#s2的reward，1\n",
    "#s3的reward,1\n",
    "#s4的reward,1\n",
    "R2_from_mdp_to_mrp = [-0.5,1,1,1]"
   ],
   "id": "c38d2422f3e87830",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:01.934953Z",
     "start_time": "2025-12-28T11:53:01.920789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "V_pi2 = compute(P=P2_from_mdp_to_mrp, rewards=R2_from_mdp_to_mrp, gamma=0.9, states_num=4)\n",
    "print(V_pi2)  #s1-s5每个状态的state value"
   ],
   "id": "8abe80316de5abc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.5]\n",
      " [10. ]\n",
      " [10. ]\n",
      " [10. ]]\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
