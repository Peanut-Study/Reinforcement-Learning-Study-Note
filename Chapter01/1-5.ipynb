{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 用蒙特卡洛方法(Monte Carlo method)求解某个策略下每个状态的state value",
   "id": "500a8c3ad8bd4fb5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 轮盘赌采样\n",
    "A被选择的概率是0.3，B被选择的概率是0.6，C被选择的概率是0.1.\n",
    "<br>从中按概率选取一个"
   ],
   "id": "80ed3802da02541e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:26.598132Z",
     "start_time": "2025-12-28T11:53:26.575418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "probabilities = {'A': 0.3, 'B': 0.6, 'C': 0.1}\n",
    "r = random.random()  # 生成[0,1)的随机数，比如0.45\n",
    "temp=0\n",
    "for i in probabilities:\n",
    "    temp += probabilities[i]\n",
    "    if temp >= r:\n",
    "        print(i)\n",
    "        break"
   ],
   "id": "6d5b2b720b0adba1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 问题描述\n",
    "如图对应《动手学强化学习》第三章Page26,求出某个策略下每个状态s的state value"
   ],
   "id": "8c04b01c308af642"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"./picture1_2.png\" alt=\"插入图片哈哈\" width=\"50%\">",
   "id": "c2a6b484849cb021"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 根据上图创建环境",
   "id": "61ec4a36b3ec9729"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:26.611060Z",
     "start_time": "2025-12-28T11:53:26.601159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "S = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态集合\n",
    "A = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作集合\n",
    "# 状态转移函数 , 这里的前往是确定性的状态转移，概率前往是随机选状态转移，对应赵书Page3\n",
    "P = {\"s1-保持s1-s1\": 1.0,\n",
    "    \"s1-前往s2-s2\": 1.0,\n",
    "    \"s2-前往s1-s1\": 1.0,\n",
    "    \"s2-前往s3-s3\": 1.0,\n",
    "    \"s3-前往s4-s4\": 1.0,\n",
    "    \"s3-前往s5-s5\": 1.0,\n",
    "    \"s4-前往s5-s5\": 1.0,\n",
    "    \"s4-概率前往-s2\": 0.2,\n",
    "    \"s4-概率前往-s3\": 0.4,\n",
    "    \"s4-概率前往-s4\": 0.4}\n",
    "# 奖励函数\n",
    "R = {\"s1-保持s1\": -1,\n",
    "    \"s1-前往s2\": 0,\n",
    "    \"s2-前往s1\": -1,\n",
    "    \"s2-前往s3\": -2,\n",
    "    \"s3-前往s4\": -2,\n",
    "    \"s3-前往s5\": 0,\n",
    "    \"s4-前往s5\": 10,\n",
    "    \"s4-概率前往\": 1}\n",
    "gamma = 0.5  # 折扣因子\n",
    "MDP = (S, A, P, R, gamma)"
   ],
   "id": "9c9bfa72418b6fc7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Policy1,和1-2中Policy1相同",
   "id": "cd407024dbe866f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:26.641186Z",
     "start_time": "2025-12-28T11:53:26.628791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 策略1,随机策略\n",
    "Pi_1 = {\"s1-保持s1\": 0.5,\n",
    "        \"s1-前往s2\": 0.5,\n",
    "        \"s2-前往s1\": 0.5,\n",
    "        \"s2-前往s3\": 0.5,\n",
    "        \"s3-前往s4\": 0.5,\n",
    "        \"s3-前往s5\": 0.5,\n",
    "        \"s4-前往s5\": 0.5,\n",
    "        \"s4-概率前往\": 0.5}"
   ],
   "id": "f59e4992409ac413",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 蒙特卡罗方法求解该则策略下的state value",
   "id": "fa8767b48fbabc3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:26.665957Z",
     "start_time": "2025-12-28T11:53:26.654363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def join(str1, str2):\n",
    "    return str1 + '-' + str2"
   ],
   "id": "86549447ec40b560",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:26.693431Z",
     "start_time": "2025-12-28T11:53:26.678681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sample(MDP, Pi, timestep_max, number):\n",
    "    ''' 采样函数,策略Pi,限制最长时间步timestep_max,number表示总共采样多少个eposides '''\n",
    "    S, A, P, R, gamma = MDP\n",
    "    episodes = []\n",
    "    for _ in range(number):\n",
    "        episode = [] #每个eposide的内容就是[(s, a, r, s_next)，(s, a, r, s_next)...]\n",
    "        timestep = 0\n",
    "        s = S[np.random.randint(4)]  # 随机选择一个除s5以外的状态s作为起点\n",
    "        # 当前状态为终止状态或者时间步太长时,一次采样结束\n",
    "        while s != \"s5\" and timestep <= timestep_max:\n",
    "            timestep += 1\n",
    "            rand, temp = np.random.rand(), 0\n",
    "            # 在状态s下根据策略选择动作\n",
    "            for a_opt in A:\n",
    "                temp += Pi.get(join(s, a_opt),0) #组合一个state和action,state前面已经选了，这里要选一个action,如果这个策略存在则就是它的概率，如果不存在就是0，后面加上0也没说\n",
    "                if temp > rand:                  #按概率选取pi\n",
    "                    a = a_opt                    #确定这个action\n",
    "                    r = R.get(join(s, a), 0)     #确定这个state,action获得的reward\n",
    "                    break\n",
    "\n",
    "            rand, temp = np.random.rand(), 0\n",
    "            # 根据状态转移概率得到下一个状态s_next\n",
    "            for s_opt in S:\n",
    "                temp += P.get(join(join(s, a), s_opt), 0)\n",
    "                if temp > rand:                  #根据概率，s,a->s_next,s,a已经确定，根据概率到达s_next\n",
    "                    s_next = s_opt               #确定这个s_next\n",
    "                    break\n",
    "            episode.append((s, a, r, s_next))    #把（s,a,r,s_next）元组放入序列中\n",
    "            s = s_next  # s_next变成当前状态,开始接下来的循环\n",
    "        episodes.append(episode)\n",
    "    return episodes\n",
    "\n",
    "# 采样5次,每个序列最长不超过20步\n",
    "episodes = sample(MDP, Pi_1, 20, 5)\n",
    "print('第一条序列\\n', episodes[0])\n",
    "print('第二条序列\\n', episodes[1])\n",
    "print('第五条序列\\n', episodes[4])"
   ],
   "id": "5c84b60777f62359",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一条序列\n",
      " [('s3', '前往s5', 0, 's5')]\n",
      "第二条序列\n",
      " [('s3', '前往s5', 0, 's5')]\n",
      "第五条序列\n",
      " [('s2', '前往s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '前往s5', 10, 's5')]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:26.740358Z",
     "start_time": "2025-12-28T11:53:26.709711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 对所有采样序列计算所有状态的价值\n",
    "def MC(episodes, V, N, gamma):\n",
    "    for episode in episodes:\n",
    "        G = 0\n",
    "        for i in range(len(episode) - 1, -1, -1):  #一个序列从后往前计算\n",
    "            (s, a, r, s_next) = episode[i]\n",
    "            G = r + gamma * G\n",
    "            N[s] = N[s] + 1                  #这个state访问次数+1\n",
    "            V[s] = V[s] + (G - V[s]) / N[s]  #更新这个state value\n",
    "\n",
    "timestep_max = 20\n",
    "# 采样1000次,可以自行修改\n",
    "episodes = sample(MDP, Pi_1, timestep_max, 2000)\n",
    "gamma = 0.5\n",
    "V = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}   #表示初始化每个状态的state value\n",
    "N = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}   #表示初始化每个状态的访问次数\n",
    "MC(episodes, V, N, gamma)\n",
    "print(\"使用蒙特卡洛方法计算MDP的状态价值state value为\\n\", V)"
   ],
   "id": "1db8e8b6b0086758",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用蒙特卡洛方法计算MDP的状态价值state value为\n",
      " {'s1': -1.2173046676569945, 's2': -1.6863598535054698, 's3': 0.5163934921636807, 's4': 6.093268018889927, 's5': 0}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:26.772358Z",
     "start_time": "2025-12-28T11:53:26.757358Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7ca914619ec06926",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch01)",
   "language": "python",
   "name": "torch01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
