{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 求每个状态的state value（用迭代法求解贝尔曼方程，得到state value，数值解）",
   "id": "26c675418cede5fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 问题描述\n",
    "如图对应《强化学习的数学原理》第二章2.5示例Page22,求出某个策略下每个状态s的state value"
   ],
   "id": "57bc3a8e2297aaba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"./picture1_3.png\" alt=\"插入图片哈哈\" width=\"40%\">",
   "id": "dd6794f23a9a30cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 根据上图创建环境",
   "id": "34f39ed9075a6d60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:15.031555Z",
     "start_time": "2025-12-28T11:53:15.015884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "S = [\"s1\", \"s2\", \"s3\", \"s4\"]  # 状态集合\n",
    "A = [\"a1\", \"a2\", \"a3\", \"a4\", \"a5\"]  # 动作集合,上，下，左，右，原地\n",
    "# 状态转移函数\n",
    "P = {   \"s1-a1-s1\": 1.0,\n",
    "        \"s1-a2-s2\": 1.0,\n",
    "        \"s1-a3-s3\": 1.0,\n",
    "        \"s1-a4-s1\": 1.0,\n",
    "        \"s1-a5-s1\": 1.0,\n",
    "\n",
    "        \"s2-a1-s2\": 1.0,\n",
    "        \"s2-a2-s2\": 1.0,\n",
    "        \"s2-a3-s4\": 1.0,\n",
    "        \"s2-a4-s1\": 1.0,\n",
    "        \"s2-a5-s2\": 1.0,\n",
    "\n",
    "        \"s3-a1-s1\": 1.0,\n",
    "        \"s3-a2-s4\": 1.0,\n",
    "        \"s3-a3-s3\": 1.0,\n",
    "        \"s3-a4-s3\": 1.0,\n",
    "        \"s3-a5-s3\": 1.0,\n",
    "\n",
    "        \"s4-a1-s2\": 1.0,\n",
    "        \"s4-a2-s4\": 1.0,\n",
    "        \"s4-a3-s4\": 1.0,\n",
    "        \"s4-a4-s3\": 1.0,\n",
    "        \"s4-a5-s4\": 1.0}\n",
    "# 奖励函数\n",
    "R = {\"s1-a1\": 0,\n",
    "    \"s1-a2\": -1,\n",
    "    \"s1-a3\": 0,\n",
    "    \"s1-a4\": 0,\n",
    "    \"s1-a5\": 0,\n",
    "    \"s2-a1\": -1,\n",
    "    \"s2-a2\": -1,\n",
    "    \"s2-a3\": 1,\n",
    "    \"s2-a4\": 0,\n",
    "    \"s2-a5\": -1,\n",
    "    \"s3-a1\": 0,\n",
    "    \"s3-a2\": 1,\n",
    "    \"s3-a3\": 0,\n",
    "    \"s3-a4\": 0,\n",
    "    \"s3-a5\": 0,\n",
    "    \"s4-a1\": -1,\n",
    "    \"s4-a2\": 0,\n",
    "    \"s4-a3\": 0,\n",
    "    \"s4-a4\": 0,\n",
    "    \"s4-a5\": 1}\n",
    "gamma = 0.5  # 折扣因子\n",
    "MDP = (S, A, P, R, gamma)"
   ],
   "id": "ced8628230b64634",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Policy1(对应P22例子1)",
   "id": "b25c2678a87a694b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:15.047530Z",
     "start_time": "2025-12-28T11:53:15.037358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Pi_1 = {\"s1-a3\": 1,\n",
    "        \"s2-a3\": 1,\n",
    "        \"s3-a2\": 1,\n",
    "        \"s4-a5\": 1,}   #这是一个确定性策略 deterministic policy"
   ],
   "id": "735b029cd5bb1d12",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 迭代法求解Policy1情况下的state value",
   "id": "1046cf7dbe518a81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:15.077852Z",
     "start_time": "2025-12-28T11:53:15.064023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "P_from_mdp_to_mrp=[\n",
    "    [0,0,1,0],\n",
    "    [0,0,0,1],\n",
    "    [0,0,0,1],\n",
    "    [0,0,0,1]\n",
    "]\n",
    "#转成二维矩阵\n",
    "P1 = np.array(P_from_mdp_to_mrp)"
   ],
   "id": "5cfc97d69e750b12",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:15.105566Z",
     "start_time": "2025-12-28T11:53:15.094330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#根据策略1，写出奖励\n",
    "#s1的reward，0\n",
    "#s2的reward，1\n",
    "#s3的reward,1\n",
    "#s4的reward,1\n",
    "R1 = [0,1,1,1]"
   ],
   "id": "fb4c511965c7731b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:15.136563Z",
     "start_time": "2025-12-28T11:53:15.123546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#对应《强化学习的数学原理》Page27，数值解\n",
    "def policy_evaluation(P, R, gamma, theta):\n",
    "    \"\"\"\n",
    "    数值迭代求解策略下状态价值函数 V(s)\n",
    "    参数:\n",
    "    P : ndarray, shape (n_states, n_states)\n",
    "        策略下的状态转移矩阵\n",
    "    R : ndarray, shape (n_states,)\n",
    "        策略下的状态奖励向量\n",
    "    gamma : float\n",
    "        折扣因子\n",
    "    theta : float\n",
    "        收敛阈值\n",
    "    返回:\n",
    "    V : ndarray, shape (n_states,)\n",
    "        各状态的状态价值\n",
    "    \"\"\"\n",
    "    n_states = P.shape[0]\n",
    "    V = np.zeros(n_states)\n",
    "    delta = float('inf')\n",
    "\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        V_new = np.copy(V)\n",
    "        for s in range(n_states):\n",
    "            V_new[s] = R[s] + gamma * np.sum(P[s] * V)\n",
    "            delta = max(delta, abs(V_new[s] - V[s]))\n",
    "        V = V_new\n",
    "\n",
    "    return V"
   ],
   "id": "5a96c1b09d5d28d2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:15.163985Z",
     "start_time": "2025-12-28T11:53:15.149466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "V_pi1 = policy_evaluation(P1, R1, 0.9, theta=1e-7)\n",
    "print(V_pi1)"
   ],
   "id": "9de79ce64c69b0df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.9999991 9.9999991 9.9999991 9.9999991]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Policy2(对应P24例子2)",
   "id": "7de5a5b1776e1991"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:15.192916Z",
     "start_time": "2025-12-28T11:53:15.182687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Pi_2 = {\"s1-a3\": 0.5,\n",
    "        \"s1-a2\": 0.5,\n",
    "        \"s2-a3\": 1,\n",
    "        \"s3-a2\": 1,\n",
    "        \"s4-a5\": 1}   #这是一个随机选策略 stochastic policy"
   ],
   "id": "e328272bef0e9cae",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 迭代法求解Policy2情况下的state value",
   "id": "383a04cdc741f2c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:15.225165Z",
     "start_time": "2025-12-28T11:53:15.210997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "P2_from_mdp_to_mrp=[\n",
    "    [0,0.5,0.5,0],\n",
    "    [0,0,0,1],\n",
    "    [0,0,0,1],\n",
    "    [0,0,0,1]\n",
    "]\n",
    "#转成二维矩阵\n",
    "P2 = np.array(P2_from_mdp_to_mrp)"
   ],
   "id": "137367235d33f1c8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:15.256365Z",
     "start_time": "2025-12-28T11:53:15.242704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#根据策略2，写出奖励\n",
    "#s1的reward，0.5*(-1)+0.5*0=-0.5\n",
    "#s2的reward，1\n",
    "#s3的reward,1\n",
    "#s4的reward,1\n",
    "R2 = [-0.5,1,1,1]"
   ],
   "id": "c38d2422f3e87830",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:53:15.287170Z",
     "start_time": "2025-12-28T11:53:15.274123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "V_pi2 = policy_evaluation(P2, R2, 0.9, theta=1e-10)\n",
    "print(V_pi2)"
   ],
   "id": "8abe80316de5abc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.5 10.  10.  10. ]\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch01)",
   "language": "python",
   "name": "torch01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
